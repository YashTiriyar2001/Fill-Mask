# -*- coding: utf-8 -*-
"""Mask_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q1lGiSrbZlIX2FCZtHJqBbO6LAajeP2e
"""

!pip install transformers
!pip install datasets
!apt install git-lfs
!git config --global user.email "you@example.com"
!git config --global user.name "Your Name"

from google.colab import drive
drive.mount('/content/drive')

from datasets import load_dataset
from transformers import AutoTokenizer
from transformers import TFAutoModelForMaskedLM
from transformers import create_optimizer, AdamWeightDecay
import tensorflow as tf
from transformers import DataCollatorForLanguageModeling



class mask:
  def load_dataset(self, conf):
    datasets = load_dataset(conf["dataset1"], conf["dataset2"])
    return datasets


  def tokenizer(self, datasets, conf):
    def tokenize_function(examples):
      return tokenizer(examples["text"])
    tokenizer = AutoTokenizer.from_pretrained(conf["model"])
    tokenized_datasets = datasets.map(
      tokenize_function, batched=True, num_proc=4, remove_columns=["text"]
    )
    return tokenized_datasets, tokenizer


  def lm_datasets(self, tokenized_datasets, conf):
    #CausalLM models
    block_size = conf["block_size"]
  
    def group_texts(examples):
      # Concatenate all texts.
      concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
      total_length = len(concatenated_examples[list(examples.keys())[0]])

      total_length = (total_length // block_size) * block_size
      # Split by chunks of max_len.
      result = {
          k: [t[i : i + block_size] for i in range(0, total_length, block_size)]
          for k, t in concatenated_examples.items()
      }
      result["labels"] = result["input_ids"].copy()
      return result
    lm_datasets = tokenized_datasets.map(
      group_texts,
      batched=True,
      batch_size=1000,
      num_proc=4,
    )
    return lm_datasets


  def model(self, conf, lm_datasets, tokenizer):
    model = TFAutoModelForMaskedLM.from_pretrained(conf["model"])
    optimizer = AdamWeightDecay(lr=2e-5, weight_decay_rate=0.01)
    model.compile(optimizer=optimizer, jit_compile=True)
    data_collator = DataCollatorForLanguageModeling(
      tokenizer=tokenizer, mlm_probability=0.15, return_tensors="np"
    )

    train_set = model.prepare_tf_dataset(
      lm_datasets["train"],
      shuffle=True,
      batch_size=16,
      collate_fn=data_collator,
    )

    validation_set = model.prepare_tf_dataset(
      lm_datasets["validation"],
      shuffle=False,
      batch_size=16,
      collate_fn=data_collator,
    )
    model.fit(train_set, validation_data=validation_set, epochs=6)
    print("Model trained sucessfully...............................................................")
    return model

  def run(self, conf):
    datasets = self.load_dataset(conf)
    tokenized_datasets, tokenizer = self.tokenizer(datasets, conf)
    lm_datasets = self.lm_datasets(tokenized_datasets, conf)
    model = self.model(conf, lm_datasets, tokenizer)
    model.save_pretrained('/content/drive/MyDrive/Accure.ai/Mask_model/model')
    tokenizer.save_pretrained('/content/drive/MyDrive/Accure.ai/Fine-tune_NLP/tokenizer')

conf = {}
conf["dataset1"] = "wikitext"
conf["dataset2"] = "wikitext-2-raw-v1"
conf["model"] = "distilroberta-base"
conf["block_size"] = 128
mask_model = mask()
mask_model.run(conf)

